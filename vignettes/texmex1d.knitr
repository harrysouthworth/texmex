\documentclass[10pt]{article}
%*********************************************************************
%*********************************************************************
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{bbm}
\usepackage{cite}

%\VignetteIndexEntry{texmex1d}
%\VignetteEngine{knitr}

%*********************************************************************

<<include=FALSE>>=
library(knitr)
library(gridExtra)
opts_chunk$set(fig.path='AutoGeneratedFiles/texmex1d',
               dev = 'png',dpi=200,warning=FALSE,message=FALSE)
@

%*********************************************************************
%DEFINE SYMBOLS
\def\IR{\hbox{{\rm I}\kern -0.2em\hbox{{\rm R}}}}
\def\bX{\boldsymbol X}
\def\bY{\boldsymbol Y}
\def\bZ{\boldsymbol Z}
\def\bc{\boldsymbol c}
\def\bd{\boldsymbol d}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
%*********************************************************************
\begin{document}

<<include=FALSE>>=
opts_chunk$set(concordance=TRUE)
@


\title{Univariate Extreme Value Modelling  using R}
\author{Harry Southworth and Janet E.\ Heffernan}
\maketitle

\setkeys{Gin}{width=1.0\textwidth}
%
\section{Introduction}
%
This document illustrates the use of the {\tt texmex} package
for performing extreme value analysis of some environmental data in {\tt R},~\cite{R}.  This package vignette focusses on univariate extreme value modelling of threshold excesses using the generalized Pareto distribution (GPD), and of data arising as maxima, using the generalized extreme value (GEV) distribuion.  The separate vignette {\tt texmexMultivariate} examines multivariate extreme value
modelling using a conditional threshold based approach.  For extreme value modelling of temporally dependent Peaks over Threshold data by using declustering, see the package vignette {\tt declustering}.

To cite this vignette, refer to Vignette name: {\tt texmex1d} and use the package citation:
<<cite,echo=FALSE,fig.keep='none'>>=
citation("texmex")
@
Extreme value statistical models are unusual among statistical models in that they are often required for extrapolation beyond levels observed in the data.  As statisticians, we are told that extrapolation from statistical models is perilous: our models can only be trusted in regions where we have sufficient data to calibrate and check goodness of model fit.  Extreme value modelling has responded to a demand for extrapolation beyond this safe region.  Since we can no longer rely on data as a check on our models' suitability, extreme value statisticians turn to mathematical arguments to bolster their confidence in their extrapolation.  These arguments provide a justification for the use of a particular type of model to describe tail behaviour of random variables.

This is not a tutorial in Extreme Value Theory, for which we refer the reader to \cite{coles}, which describes a range of methods for modelling the statistical properties of sample maxima, threshold excesses, extremes of dependent series and other aspects of tail behaviour.

%
\subsection{Preliminaries}
%
With {\tt texmex} installed, use the {\tt library} command to make the package available to the current session, set the colours used for graphics, and set the random seed so that results are reproducible on a given machine:
<<setstuff>>=
library(texmex)
library(gridExtra)
palette(c("black","purple","cyan","orange"))
set.seed(20130618)
@
The {\tt gridExtra} package is used for laying out plots produced by {\tt ggplot2}.
%
\subsection{Data}
%
The datasets used in this example analysis are contained in the {\tt texmex}
package. We give a detailed exposition of the fitting of the GPD without covariates to a daily rainfall dataset, {\tt rain}, which appears in Coles (2001),~\cite{coles}.  We show how to extend the modelling framework to include covariates using the {\tt winter} air pollution data from Heffernan and Tawn (2004),~\cite{heffernanTawn}.  The modelling approach for fitting GEV models that we take within {\tt texmex} is very similar to that for fitting the GPD, so we conclude with a brief demonstration of this by using the annual maxima sea-level dataset, {\tt portpirie}, again from Coles~\cite{coles}.  More details of these datasets are given in their help files.
\subsubsection{Rainfall data}
<<data.rain>>=
head(rain)
summary(rain)
length(rain)
@

To plot the data (not shown here):
<<data.rain.plot,eval=FALSE,echo=TRUE>>=
d1 <- ggplot(data=data.frame(rain=rain,index=1:length(rain)),
             aes(index,rain)) +
    geom_point(alpha=0.5,col=4)
d1
@
\subsubsection{Winter air pollution data}
\label{section:winterData}
<<data.winter>>=
head(winter)
summary(winter,digits=3)
dim(winter)
@
We focus on the two variables Nitrogen Dioxide, {\tt NO2} and ozone {\tt O3} in the examples of covariate modelling that follow.  A scatter plot of these two variables suggests a negative association:
<<data.winter2d>>=
d2 <- ggplot(winter,aes(O3,NO2)) + geom_point(alpha=0.5,col=4)
d2
@
\subsubsection{Portpirie data}
<<data.portpirie>>=
head(portpirie)
summary(portpirie)
dim(portpirie)
d3 <- ggplot(portpirie,aes(Year,SeaLevel)) + geom_point(col=4)
d3
@

%****************************************************************
%****************************************************************
%
\section{Generalized Pareto distribution models}
%
We now proceed to fit, evaluate, choose between, and ultimately
make predictions from generalized Pareto distribution (GPD) models.
%
\subsection{Extreme value modelling and asymptotic motivation for the GPD}
%
In this section, we show how to fit the generalised Pareto Distribution,
GPD($\sigma, \xi$)~\cite{davisonSmith} to
data points in excess of suitably chosen thresholds.
The GPD has distribution function
\begin{eqnarray}
\label{eqn:genParetoDistnFun}
F_{>u}(x)=1-\left\{1+\xi\left(\frac{x-u}{\sigma}\right)\right\}^{-1/\xi}
\mbox{ for }x>u,
\end{eqnarray}
where $u$ is the threshold for fitting and $\sigma>0$ and $\xi \in
\IR$ are the scale and shape parameters respectively.  This is the
conditional distributon of observations given that the observations
exceed the fitting threshold $u$. The range of possible values taken
by realisations from the GPD depends on the parameter values, with the
distribution having a finite upper end point (short tailed) if the
shape parameter is negative ($u<x\leq u-\sigma/\xi$ if $\xi<0$) and an
infinite tail otherwise ($u<x<\infty$ if $\xi\geq 0$).  When $\xi=0$, the GPD corresponds exactly to the Exponential distribution.

Extreme value theory tells us that under appropriate normalisation of
the threshold excesses, as the threshold $u$ tends to the
distributional upper endpoint, the limiting distribution of the
excesses must fall in the generalised Pareto family of distributions
(given certain conditions concerning non-degeneracy of the limit
distribution and smoothness of the distribution of the original
variable). So whatever the original distribution of the measurements,
provided we choose an appropriately high threshold, the distribution
of values exceeding that threshold should be well approximated by a
GPD.  Diagnostic tools to aid the choice of suitable threshold are
standard, and are described shortly -- see also~\cite{coles}.
%
\subsection{Parameterization}
\label{sect:parameterization}
%
The usual parameterization of the GPD (as in Equation~(\ref{eqn:genParetoDistnFun})) is in terms of its
scale paramter $\sigma$ and shape parameter $\xi$. There
are, however, good reasons for reparameterizing in terms
of $\phi = \log\sigma$:
\begin{itemize}
\item Experience has demonstrated that the numerical
algorithms used for optimizing the log-likelihood tend
to converge more reliably when working with $\phi$;
\item When including covariates in the model we are
faced with the constraint that $\sigma > 0$ and working
with a linear predictor specified in terms of $\phi = \log\sigma$
guarantees this constraint;
\item When placing prior distributions on parameters,
it is convenient to work with Gaussian distributions and
$\phi$ is more likely to be close to Gaussian than is
$\sigma$.
\end{itemize}
As such, some of the functions in {\tt texmex} work with $\phi$, not
$\sigma$. In the case when inference is required for $\sigma$ rather than $\phi$, the point estimates
can simply be exponentiated if
maximum likelihood estimation is used. If a prior distribution is
used, the point estimates are not invariant to transformation,
so any transformed values should only be considered to
be approximate.
%
\subsection{Threshold selection}
%
GPD modelling proceeds by selecting a threshold above
which the data appear to be well modelled. Standard
tools for threshold selection that appear in the
literature (see for example~\cite{coles}) include the \emph{mean residual
life} (MRL) plot, and plots of parameters estimated using a range of
thresholds, \emph{threshold stability plots}.

For a suitably chosen threshold, the mean residual life
plot should be linear and the parameter estimates in threshold stability plots
constant above the chosen threshold (both of these requirements are assessed by taking account of sampling variability). The sign of the gradient in the linear part of the MRL plot corresponds to the sign of the shape parameter and hence indicates the shape of the tail -- negative slope shows a short tailed distribution, a horizontal line (zero gradient) shows an exponential type tail and a positive slope suggests a heavy tailed distribution.

We illustrate the use of these diagnostics now for the {\tt rain} data:
<<threshold>>=
grfRain <- gpdRangeFit(rain,umax=35)
mrlRain <- mrl(rain)

g1 <- ggplot(grfRain)
g2 <- ggplot(mrlRain)

grid.arrange(g1[[1]] + ggtitle("Stability plot, scale parameter"),
             g1[[2]] + ggtitle("Stability plot, shape parameter"),
             g2 + ggtitle("MRL plot"),ncol=2)
@
The threshold stability plots (top) show both (log-)scale and shape parameter estimates to be stable for thresholds of around 20 and above.

The Mean Residual Life plot (bottom) has a linear form from values of around ten, the gradient being positive (up to a threshold of around 60 above which there are only a small handful of points).  This indicates a heavy tail and positive shape parameter.

Note that this form of MRL plot is typical, with the very highest thresholds giving very erratic estimates with apparently narrow confidence bands.  This commonly observed feature is due to the estimates for very high thresholds being based on a very small number of points - the very largest points in the data set.  These are by construction close to the sample maximum and therefore MRL plots often have a sudden negative slope for very high values of threshold, which can be spurious as in this case.

For our example, a threshold around 20 therefore appears to be sensible. However, we will need to
do some additional diagnostics to check this.
We proceed by selecting the $97^{th}$ percentile
as being the candidate threshold.
<<thresholdChoice>>=
quantile(rain,0.97)
@
The theory underpinning the GPD tells us that (if the underlying distribution satisfies our conditions) there exists a
threshold above which the GPD fits the data, but the
theory does not specify that the threshold necessarily must be
high.  Indeed, if the data are realisations from an Exponential distribution -- which is a member of the GPD class, with shape parameter $\xi=0$ --  then a threshold equal to the minimum data point would be appropriate.  In many cases of course, the threshold will be towards the top end of the observed data range, the motivation for the GPD as a tail model being asymptotic as the threshold goes to infinity.  If sample sizes are too small, it may be the case that a suitable threshold cannot be chosen from within the range of the data with any degree of confidence.
%
\subsection{GPD fitting in {\tt texmex}}
%

The generalised Pareto model for threshold excesses can
be fit by using the {\tt texmex} function {\tt evm}, {\it Extreme Value Model}, which has a default {\tt family=gpd} argument.  We must specify the threshold to be used for fitting:

For the {\tt rain} data:
<<evmGPD>>=
rain.fit <- evm(rain,th=20)
rain.fit
@
The estimated shape parameter shows us that with this threshold, the fitted GPD has a heavy tail, as $\hat\xi>0$.  This is in line with our expectations following inspection of the MRL plot.

We examine diagnostics plots to see whether these support our initial choice of threshold:
<<plotGPD>>=
ggplot(rain.fit)
@
The shaded regions in the P-P and Q-Q plots indicate pointwise 95\% tolerance
intervals, based on 1000 simulated datasets. The shaded region in the return level plot shows 95\% pointwise confidence intervals, based on a normal approximation.

The fit shown in these plots is good, with data and model agreeing well over the range of threshold exceedances.  We can see how a lower threshold (perhaps that suggested by the MRL plot but not the threshold stability plot) gives an estimated slightly less heavy tail and a poorer fit:
<<plotGPDpoorFit>>=
ggplot(evm(rain,th=10))
@
%
\subsection{Maximum penalized likelihood estimation}
\label{section:MPLE}
%
With small sample sizes, the GPD log-likelihood function
often becomes flat and the optimiser can fail to converge.
One way to overcome this is to penalize the likelihood by
some function of the parameters. Experience suggests that
the main problems may be overcome by putting fairly modest
penalties on $\xi$.

Thus, rather than maximize the log-likelihood $l(\phi, \xi | X)$
we maximize
\begin{eqnarray}
l(\phi, \xi) - \lambda \xi^2 \label{eqn:MPL}
\end{eqnarray}
for some $\lambda$.

\subsubsection{Choice of $\lambda$}
If we exponentiate (\ref{eqn:MPL}), the result can be
written as
\begin{eqnarray}
L(\phi, \xi | X) e^{-\xi^2/2\theta^2} \label{eqn:LP}
\end{eqnarray}
in which $\theta = \sqrt{\frac{1}{2\lambda}}$. The rightmost
term in (\ref{eqn:LP}) is proportional to a Gaussian
distribution centred at 0. Thus, maximum penalized
likelihood estimation has a Bayesian interpretation
and corresponds to maximum a posteriori estimation.

For the GPD, $\xi = -1$ corresponds to the distribution
being uniform, $\xi = 0$ corresponds to it being
exponential, and $\xi = 1$ corresponds to it being so
heavy-tailed that its expectation is infinite. For many applications, values of $\xi = -1$ and $\xi = 1$ may be
implausible, and we would expect values of $\xi$ to be
fairly close to 0. This implies a prior distribution
that is Gaussian, centred at zero, with standard deviation $\theta = \frac{1}{2}$.

Since convergence issues are generally associated with
$\xi$, we can choose a diffuse prior for $\phi$, say
$\phi \sim N(0, 10^4)$.

In general, we attempt to use MLE or penalized MLE with
diffuse priors for both $\phi$ and $\xi$.
Prior distribution $\xi \sim N(0, \frac{1}{4})$
independently of a diffuse prior on $\phi$ can be
used when convergence issues arise:

<<MPLE, eval=FALSE>>=
pp <- list(c(0, 0), diag(c(10^4, .25)))
rain.pen <- evm(rain, qu=.97, priorParameters = pp, prior="gaussian")
@
in which {\tt priorParameters} is a list containing the mean $(0, 0)^T$
and covariance matrix of the prior Gaussian distribution.
%
\subsection{Covariate modelling }
\label{section:covariateModels}
%
We can fit GPD models with covariates in $\phi$,
in $\xi$, in neither, or in both. We use the {\tt winter} air pollution dataset to illustrate this model fitting, with NO2 as the response and O3 as the explanatory variable. Plots of this data in Section~\ref{section:winterData} suggested a negative association between these variables. A threshold corresponding to the 70\% quantile of the NO2 variable was chosen using the diagnostic techniques in the previous section (output not shown here).  As a starting point we fit the GPD with no covariates:

<<simplegpd>>=
airpoll <- evm(NO2, data=winter, qu=.7, penalty="none", family=gpd)
airpoll
ggplot(airpoll)
@

 The plots show no systematic departure of the data from the model at this choice of threshold, so
we proceed to fit various models with covariates:

<<gpdmods1>>==
airpoll1 <- update(airpoll, phi= ~O3, xi= ~O3)
airpoll2 <- update(airpoll, phi= ~O3)
airpoll3 <- update(airpoll, xi= ~O3)
@

The default model diagnostic plots for the model are different when there are covariates included in the model.  Here we look at diagnostics for the model with O3 included in the linear predictors for both $\phi$ and $\xi$:
<<gpdCovDiagnostics>>=
ggplot(airpoll1)
@

Since there is a covariate in the model the probability and quantile plots are constructed using the model residuals, which are exponential under the fitted model. We also have a plot of the residuals against the fitted parameters for any parameter that is modelled using a covariate (in this case the scale parameter $\phi$ and shape parameter $\xi$). A well fitting model should have homogeneity of residuals across different values of the fitted parameter.  These diagnostic plots give no cause for concern.  There are no return level or histogram/density plots produced since the estimates of these quantities depend on the precise values taken by the covariates in the model.


<<AIC>>==
AIC(airpoll)
AIC(airpoll1)
AIC(airpoll2)
AIC(airpoll3)
@
AIC is lowest for {\tt airpoll2} which has O3 as a covariate affecting the scale parameter of the GPD, but not the shape parameter.
Since {\tt airpoll2}
has the lowest AIC we prefer that model.

We now examine more detailed model diagnostics for our preferred model, {\tt airpoll2}:
<<airpoll2diag>>=
g3 <- ggplot(airpoll2,plot.=FALSE)
g4 <- ggplot(predict(airpoll2,type="lp",ci.fit=TRUE))[[1]] +
    ggtitle("Fitted (log)scale parameter")

grid.arrange(g3[[1]],g3[[2]],g3[[3]],g4,ncol=2)
summary(airpoll2)
@
The {\tt summary} command reveals an alarming number of points to lie outside of the 95\% tolerance interval constructed for the Q-Q plot; the corresponding plot shows these points to be among those lying closest to the fitting threshold.  This suggests that we should re-visit the whole model selection procedure again at a higher threshold (details not shown here).  We eventually settle on the following model, fit at a threshold corresponding to the 90\% quantile:
<<airpoll4diag>>=
airpoll4 <- update(airpoll2,qu=0.9)
g5 <- ggplot(airpoll4,plot.=FALSE)
g6 <- ggplot(predict(airpoll4,type="lp",ci.fit=TRUE))[[1]] +
    ggtitle("Fitted (log)scale parameter")

grid.arrange(g5[[1]],g5[[2]],g5[[3]],g6,ncol=2)

summary(airpoll4)
@
The fit of this model is adequate, having apparently captured the dependence of the scale parameter on O3.  The final plot shows the nature of this relationship, with larger values of O3 giving lower values of $\phi$.  This concurs with the observed negative relationship between the variables shown in the original scatter plot, Section~\ref{section:winterData}.

%
\subsection{GPD parameter uncertainty}
%
We examine briefly here Information Matrix summaries and Bootstrap estimates of GPD parameter uncertainty, before going on to use a Bayesian simulation based approach to estimation of our GPD model parameters and associated uncertainty.
%
\subsubsection{Information matrix based approaches}
%
When the GPD model is fit by using the default  maximum likelihood estimation, an estimate of the covariance matrix of model parameters is returned.  The default procedure for estimating this covariance matrix is {\tt cov="observed"} in which case the observed information matrix is used, as given in Appendix A of Davison and Smith~\cite{davisonSmith}. The only other option is {\tt cov = "numeric"} in which case a numerical approximation of the Hessian is used (see the help for {\tt optim}). In some cases, particularly with small samples, the numerical approximation can be quite different from the closed form ({\tt cov="observed"}) result, and the value derived from the observed information should be preferred.

For our fitted model, we compare the two approaches and find that the alternative methods give almost identical estimates of the Information matrix:
<<numericCovMat>>=
airpoll2$cov
update(airpoll2,cov="numeric")$cov
@
For small samples, the underlying log-likelihood may be far from quadratic, and the resulting estimates of standard errors derived using either of these methods are liable to approximate poorly the true standard errors.

%
\subsubsection{Parametric Bootstrap approach}
An alternative approach to uncertainty estimation is to use a parametric bootstrap -- which does capture the asymmetry of the log-likelihood surface around the maximum likelihood estimates.  This is carried out for our fitted model in {\tt texmex} as follows:
<<optionsWarningOff,eval=TRUE,echo=FALSE,tidy=TRUE>>=
o <- options(warn=-1)
@
<<bootstrap,message=FALSE>>=
boot <- evmBoot(airpoll2, trace=1001)
summary(boot)
@
We can compare these reported standard deviations with the correponding estimates derived from the Observed Information matrix estimate -- these are close although not identical, with the largest disagreement occurring for the shape parameter.  This is typical behaviour of the GPD model.
<<CompareBootSdWithObsInfo>>=
sqrt(diag(airpoll2$cov))
@
We can also compare the bootstrap based estimate of the parameter correlation matrix with that derived from the Observed Information matrix:
<<InfoMatCorr,message=FALSE>>=
cov2cor(airpoll2$cov)
cov2cor(summary(boot)$covariance)
@
<<optionsWarningOn,eval=TRUE,echo=FALSE,tidy=TRUE>>=
options(o)
@
Estimates of this correlation matrix are similar although not identical, as anticipated.

Focussing on the covariance matrix of the parameter estimates is misleading and does not let us explore the asymmetric nature of the uncertainty about the parameter estimates.  This can often be better seen in the bootstrap based confidence intervals for the model parameters shown in the following plots, although the asymmetry is not pronounced in this example:
<<bootstrapLinPredConfIntsLPplot>>=
O3 <- data.frame(O3=seq(10,60,len=6))

g7 <- ggplot(predict(boot,    newdata=O3,type="lp",
                     ci.fit=TRUE))[[1]] +
    ggtitle("Bootstrap")
g8 <- ggplot(predict(airpoll2,newdata=O3,type="lp",
                     ci.fit=TRUE))[[1]] +
    ggtitle("Obs Info")

grid.arrange(g7,g8,ncol=2)
@

\subsection{Bayesian estimation}
\label{sebsection:BayesEstimation}
A further alternative approach to uncertainty estimation which accurately reflects the asymmetric nature of the uncertainty is offered by Bayesian simulation based methods.  In {\tt texmex} we can simulate from the posterior distributions of the parameters by using the {\tt evm} function again, this time using {\tt method = "simulate"} to tell the function to simulate from the joint posterior distribution of the parameters.

<<gpdmcmc, eval=FALSE>>=
airpollSim <- evm(NO2, data=winter, qu=.7,
                  phi=~O3, method="simulate",
                  verbose=FALSE)
@

Equivalently, the Bayesian estimation based on MCMC can also be instigated by the use of the function {\tt update} on the previously chosen model.  The method of estimation is changed from {\tt "optimize"} -- under which estimation is carried out using (penalized) maximum likelihood -- to {\tt "simulate"} -- under which a Metropolis algorithm is used to simulate from the joint posterior distribution of the parameters.  For our preferred model, {\tt airpoll2}, this is implemented as follows:

<<update, eval=TRUE>>=
airpollSim <- update(airpoll2,method="simulate",penalty="gaussian",
                     verbose=FALSE)
ggplot(airpollSim)
airpollSim
@

The plots of the Markov chains ought to look like ``fat hairy
caterpillars'' if the algorithm has converged on its target
distribution. Also, the cumulative means of the chains should
converge, the acceptance rate should not be too high or too low, and the
autocorrelation functions should rapidly decay to zero. We
conclude from the plots that there is no evidence against convergence of our Markov chains, although we should probably thin our output further to obtain a chain that is closer to independent  (the default is to thin to every 4 observations).  Here we retain the burn-in value of 500 but now discard all but every 20th observation, resulting in an autocorrelation function which decays more rapidly to zero. This results in the retention of 2000 values after discarding the burn-in and applying the thinning.  (Note that the observations are not discarded destructively and we can use the {\tt thinAndBurn} function repeatedly to examine the impact of using different values of {\tt burn} and {\tt thin}.)

<<thinmcmcE>>=
airpollSim <- thinAndBurn(airpollSim, burn=500, thin = 20)
dim(airpollSim$param)
summary(airpollSim)
@

We can use the {\tt predict} method to obtain the linear predictors for the model parameters for each unique combination of any covariates that may be in the model.

<<simparams>>=
O3 <- data.frame(O3=seq(20,50,by=10))
predict(airpollSim,newdata=O3,type="lp")
predict(airpollSim,newdata=O3,M=1000)
@
To see linear predictors for the original dataset, simply omit the {\tt newdata} argument (output not shown):
<<simparams2,eval=FALSE>>=
predict(airpollSim,type="lp")
predict(airpollSim,M=1000)
@

Setting the argument {\tt all = TRUE} returns the linear predictors for all of the simulated parameter values in the (thinned) chains:
<<airpollParamss>>=
airpollParams <- predict(airpollSim, newdata=O3, type="lp", all=TRUE)
@
 The returned object contains a list called {\tt link} with one item for each unique value of the covariate(s).  The following shows the first five simulated values of $(\phi,\xi)$ for covariate {\tt O3 = 20}:
<<airpollSimParamsO3.10>>=
airpollParams$obj$link[[1]][1:5,]
@
%
%
\subsection{Predicted return levels}
\label{subsect:retLevelsPtEsts}
%
The general definition of an $m$-observation return level for the GPD is:
\begin{eqnarray}
x_m = u + \frac{\sigma}{\xi}\{(mp)^\xi -1 \}.\label{eqn:returnLevel}
\end{eqnarray}
Here $p$ is the probability of exceeding the GPD fitting threshold
$u$ and $m$ is a large value, so that $x_m$ is termed the
{\emph m-observation return level} and represents the maximum
value of $x$ expected to be seen in $m$ observations.

The effect of the O3 variable on the fitted GPD is seen clearly when we look at return levels and associated plots for different levels of this variable:

<<retLevelO3,message=FALSE>>=
O3 <- data.frame(O3=seq(20,50,by=10))
pred <- predict(airpoll2,newdata=O3,M=5:1000,ci.fit=TRUE)
pred$obj[c(1,496,996)]
pred$call
g9 <- ggplot(pred)
xAxis <- scale_x_continuous(breaks=c(5,20,50,200,500,1000),trans="log")
yAxis <- scale_y_continuous(limits=c(50,105))
grid.arrange(g9[[1]] + xAxis + yAxis,
             g9[[2]] + xAxis + yAxis,
             g9[[3]] + xAxis + yAxis,
             g9[[4]] + xAxis + yAxis,ncol=2)
@

The {\it Return period} is in units of {\it numbers of observations}, in this case, number of winter days. {\it Return level} is in the same units as NO2 variable to which the GPD model has been fit.

This plot shows how the different values of scale parameter affect the size of return levels associated with different return periods but not the shape of this function.  The shape parameter $\xi$ is common to each of the four models used for prediction here -- this is emphasised if we allow the axes for plotting to differ for the four levels of O3:

<<retLevelO3sameAxesFALSE>>=
grid.arrange(g9[[1]],g9[[2]],g9[[3]],g9[[4]],ncol=2)
@

We can see that the underlying shapes of the four curves are identical, the main differences emphasised here is the greater uncertainty associated with the highest value of O3 examined here, which is beyond the range observed in the dataset.

The default method for estimating these confidence intervals is to use the Information Matrix and quadratic approximation, however this can lead to poor estimates as this approach gives symmetric intervals centered on the point estimates.
If we are to extrapolate far beyond the range of the data, then it can be preferable to recognise the inherent reduction in certainty that occurs as we move away from the data where the information dwells.  This is better reflected in the asymmetric confidence/credible intervals obtained by using either the bootstrap or Bayesian simuation based approach:
<<bootstrapRetLevelConfIntsLPplot,message=FALSE>>=
O3 <- data.frame(O3=c(20,50))
M <- seq(5,1000,len=40)
g11 <- ggplot(predict(airpoll2,newdata=O3,M=M,ci.fit=TRUE),main="Obs Info")
g12 <- ggplot(predict(boot,newdata=O3,M=M,ci.fit=TRUE),main="Bootstrap")
g13 <- ggplot(predict(airpollSim,newdata=O3,M=M,ci.fit=TRUE),main="Simulated")
grid.arrange(g11[[1]] + xAxis+yAxis,g11[[2]] + xAxis+yAxis,
             g12[[1]] + xAxis+yAxis,g12[[2]] + xAxis+yAxis,
             g13[[1]] + xAxis+yAxis,g13[[2]] + xAxis+yAxis,ncol=2)
@
The asymmetry in the bootstrap based confidence intervals and the Bayesian simulation based credible intervals for high return levels at large values of the covariate is marked here.
%*********************************************************************
%
\section{Generalized Extreme Value models}
%
Whereas GPD models have an asymptotic motivation as models for threshold exceedances, the Generalised Extreme Value (GEV) distribution is derived as the limiting distribution for observations arising as maxima of IID observations.

We introduce the GEV distribution now, but refer to Coles~\cite{coles} for more details of this family of distributions, and how it arises.
%
\subsection{Extreme value modelling and asymptotic motivation for the GEV}
%
In this section, we show how to fit the generalised Extreme Value  Distribution,
GEV($\mu, \sigma, \xi$) to
data points arising as sample maxima.
The GEV has distribution function
\begin{eqnarray}
\label{eqn:GEVDistnFun}
G(x)=\exp\left[-\left\{1+\xi\left(\frac{x-\mu}{\sigma}\right)\right\}^{-1/\xi}\right],
\end{eqnarray}
\mbox{ for } $x$ satisfying $1+\xi(x-u)/\sigma>0$.  The location parameter $\mu$ satisfies $-\infty <\mu<\infty$, scale parameter $\sigma>0$ and shape parameter $\xi$ satisfies $-\infty <\xi<\infty$.  The range of possible values taken
by realisations from the GEV depends on the parameter values, with the
distribution having a finite upper end point (short tailed) if the
shape parameter is negative ($x\leq \mu-\sigma/\xi$ if $\xi<0$) and an
infinite tail otherwise ($x<\infty$ if $\xi\geq 0$).  When $\xi=0$, the GEV corresponds exactly to the Gumbel distribution.

Extreme value theory tells us that under appropriate normalisation of
the sample maxima, as the underlying sample size from which maxima are taken tends to infinity, the limiting distribution of the sample maxima must fall in the Generalised Extreme Value family of distributions
(given certain conditions concerning non-degeneracy of the limit
distribution and smoothness of the distribution of the original
variable).
%
\subsection{Parameterization}
%
As for the GPD (Section~\ref{sect:parameterization}), the usual parameterization of the GEV (equation~(\ref{eqn:GEVDistnFun})) is in terms of its
location, scale and shape parameters $\mu$, $\sigma$ and $\xi$ respectively. For the same reasons as those given for the GPD in Section~\ref{sect:parameterization}, we choose to  reparameterize in terms of $\phi = \log\sigma$.  Comments made in this section regarding this parameterization in the context of the GPD apply equally to the GEV.
%
\subsection{GEV fitting in {\tt texmex}}
%
We use the annual maxima sea-level observations in the dataset {\tt portpirie} to illustrate the fitting of the GEV in {\tt texmex}.  The function {\tt evm} (Extreme Value Model) is called, this time with the argument {\tt family=gev}, which fits the GEV model rather than the default family, GPD.  Diagnostic plots are constructed in the usual way:
<<portpirieFit>>=
port <- evm(SeaLevel,data=portpirie,family=gev)
ggplot(port)
port
@
The fit of the GEV to the portpirie sea-level annual maxima appears to be good.

There is no analogue of threshold choice for the GPD (by using MRL plots and threshold stability plots), and as such there are fewer diagnostics to support the fitting of the GEV distribution.  Where poor fit is observed, it may be due to having taken maxima of an insufficient number of observations (for example, if the data are annual maxima then it may be that one year of data is not a large enough sample size from which to draw maxima).  Sometimes, but not always, it is possible to go back to the original data from which the maxima have been constructed and take maxima of a larger number of observations. This usually results in a smaller number of observed maxima so there is an obvious trade-off between bias and variance here.

Covariates may be included in the GEV models in the same manner as illulstrated in Section~\ref{section:covariateModels} for the GPD.  As an exercise, we try fitting {\tt Year} as a covariate to the {\tt portpirie} sea level observations:
<<portpirieCov>>=
port1 <- update(port,mu=~Year)
port2 <- update(port,phi=~Year)
port3 <- update(port,xi=~I((Year-1955)/1955),start=c(coef(port),0.001))
ggplot(port3)
@
An examination of the AIC for each of these fitted models ({\tt AIC(port)} etc.) reveals there to be no evidence of an effect of Year on any of the GEV model parameters.
%
\subsection{Variants on basic GEV model fitting}
%
The various different model estimation strategies outlined for the GDP model above can be applied equally to the estimation of GEV models.  We can use MCMC to estimate the GEV model parameters, just as for the GPD in Section~\ref{sebsection:BayesEstimation}:

<<gevMCMC,eval=FALSE>>=
evm(SeaLevel,data=portpirie,family=gev,method="simulate")
@
Output may be processed and examined in the same manner as for the GPD model, and we refer to the details of Section~\ref{sebsection:BayesEstimation} for outline examples in this area.

Informative priors or penalties may be applied to model parameters as follows:
<<portpiriePenalisedLik>>=
pp <- list(c(0, 0, 0), diag(c(10^4, 10^4, .25)))
update(port, priorParameters = pp, prior="gaussian")
@
This was illustrated in more detail for the GPD model in Section~\ref{section:MPLE}.

Currently, there is no implementation in {\tt texmex} of the Observed Information Matrix estimator of the covariance matrix of parameter estimates, instead estimates of this matrix are obtained using a numerical approximation to the Hessian matrix.

Uncertainty estimation via the parametric bootstrap is carried out for our fitted model in {\tt texmex} in the same manner as for the GPD, as follows:
<<bootstrapGEV>>=
boot <- evmBoot(port, trace=1001)
summary(boot)
ggplot(boot)
@
This plot shows the bootstrap distributions of the model parameter estimates, for the GEV fitted to the {\tt portpirie} dataset.
%
\subsection{Return level estimation}
%
Quantiles of the fitted GEV distribution can be estimated by using the estimated model parameters in the following equation:
\begin{eqnarray}
x_p = \left\{
\begin{array}{ll}
  \mu - \frac{\sigma}{\xi}[1-\{-\log(1-p)\}^{-\xi}, & \mbox{ for }\xi\neq 0;\\
  \mu - \sigma\log\{-\log(1-p)\}, & \mbox{ for }\xi=0.
\end{array}
\right.
\label{eqn:GEVreturnLevel}
\end{eqnarray}
Here $p$ is the probability satisfying $G(x_p)=1-p$ (where $G(x)$ is defined in equation~(\ref{eqn:GEVDistnFun})).  This has the interpretation of $x_p$ being the {\it return level} associated with return period $1/p$. For example, for annual maxima data, it is the level which is expected to be exceeded on average once every $1/p$ years.

In {\tt texmex}, return levels are estimated for the GEV as we showed for the GPD:
<<GEVretLevel,message=FALSE>>=
portRL <- predict(port,M=seq(50,1000,by=50),ci.fit=TRUE)
g14 <- ggplot(portRL)
g14[[1]] + scale_x_continuous(trans="log",breaks=c(50,100,200,500,1000))
portRL$obj[c(1,10,20)]
@


\bibliography{texmex}
\bibliographystyle{plain}
\end{document}


