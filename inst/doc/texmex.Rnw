\documentclass[10pt]{article}
%*********************************************************************
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{bbm}
\usepackage{cite}
\usepackage{Sweave}
%*********************************************************************
\SweaveOpts{prefix.string=AutoGeneratedFiles/texmex}
%\VignetteIndexEntry{texmex package}
%*********************************************************************
%DEFINE SYMBOLS
\def\IR{\hbox{{\rm I}\kern -0.2em\hbox{{\rm R}}}}
\def\bX{\boldsymbol X}
\def\bY{\boldsymbol Y}
\def\bZ{\boldsymbol Z}
\def\bc{\boldsymbol c}
\def\bd{\boldsymbol d}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
%*********************************************************************
\begin{document}

\title{Extreme value modelling of clinical laboratory safety data using R}
\author{Harry Southworth}
\maketitle

\setkeys{Gin}{width=1.0\textwidth}
%
\section{Introduction}
%
This document illustrates the use of the {\tt texmex} package (\cite{texmex})
for performing extreme value analysis of some clinical safety
laboratory data in {\tt R},~\cite{R}. Broadly speaking, the analysis proceeds in
two steps: generalized Pareto distribution (GPD) modelling of
the marginal variables; conditional multvatiate extreme value
modelling.

Please note that this is not intended to be an R tutorial. For
that, you will have to look elsewhere.
%
\subsection{Preliminaries}
%
Before doing anything else, you will need to install the
{\tt texmex} package. Depending on your installation of
R, this can be done using the {\tt install.packages}
command in R, or by downloading the package from CRAN and
installing it from a local archive.

Once you've got {\tt texmex} installed, use the {\tt library} command to make the package available to the current session.  This vignette also makes use of the {\tt rlm} function from the {\tt MASS} package (see~\cite{MASS}) and trellis plots from the {\tt lattice} package.

<<setstuff>>=
library(texmex)
library(MASS)
library(lattice)
palette(c("black","cyan","orange","purple"))
set.seed(20111011)
@
The final command sets the random seed so that the results in this vignette may be reproduced exactly.
%
\subsection{\tt texmex}
%
The {\tt texmex} package for R was written by Harry Southworth
and Janet E. Heffernan. The work was funded by AstraZeneca.

Some considerable effort has been made to ensure that the package
does what it ought to, and to this end over 250 tests are
built into the package. The tests compare the output of
functions in {\tt texmex} with published results and, where
no published results were available, with output from independently
written code. There are also logical tests (e.g.\ of use of informative prior distributions) and tests of the structures of
objects.

To test the package functionality at any time,
use the {\tt RUnit} package. With {\tt RUnit} and {\tt texmex}
attached, you can perform all the tests, and view a report
describing the results, as follows.

<<validate, eval = FALSE>>=
library(RUnit)
pdf("texmexValidation.pdf")
res <- validate.texmex()
dev.off()
printHTMLProtocol(res, "texmexValidation.html")
@
Many of the plots produced in the {\tt texmexValidation.pdf} file reproduce figures appearing in published material, and are intended for comparison against these figures.  The validation versions of the plots produced by {\tt texmex} are labelled with the target figure references.

Due to the large number of tests performed, including tests
of bootstrap and MCMC procedures, {\tt validate.texmex} takes
quite a while to run.
%
\subsection{Data}
%
The dataset used in this example analysis is contained in the {\tt texmex}
package. The dataset is called {\tt liver} and we can look at
the first few lines and a summary as follows:
<<data>>=
head(liver)
summary(liver)
@

The response variables are
\begin{description}
\item[ALT] alanine aminotransferase
\item[AST] aspartate aminotransferase
\item[ALP] alkaline phosphatase
\item[TBL] total bilirubin
\end{description}
The variables suffixed by {\tt .B} are data measured
at baseline (prior to treatment); an {\tt .M} indicates
post-baseline measurement (on treatment).
In this study, there was only one post-baseline visit. In
general, in trials which have more than one post-baseline visit it is natural to use the maximum post-baseline value for each individual.

The doses were equally spaced on a logarithmic scale (i.e.\
dose D is twice dose C, dose C is twice dose B, and dose B
is twice dose A). Later in the analysis, we will 
do some modelling with $\log(\mbox{dose})$ appearing in the linear predictor, so we
take a copy of the dataset and create a new variable now:
<<logdose>>=
liver <- liver
liver$ndose <- as.numeric(liver$dose)
table(liver$ndose)
@

Before proceeding, first note that there are two small outliers
in {\tt $ALP.M$}: two cases with {\tt $ALP.M = 1$}. Discussion with the study
physician led to the conclusion that the lowest value of ALP 
in the dataset was impossible, and since it is large values in which 
we are interested anyway, we can remove it (leaving it
in complicates plotting later in the analysis, but does not affect
any of the results).

<<cleanData>>=
liver <- liver[liver$ALP.M > 1,]
@

The variables in the dataset all relate to the liver.
Biologically, the understanding is that liver cells release
ALT and AST as they die. If a sufficient amount of the liver is
destroyed that it can no longer function properly, then
it ceases to be able to clear bilirubin and so bilirubin
(TBL) goes up. The situation is complicated by the fact that
ALT and AST can also arise from other sources (e.g.\ muscle),
and that ALP can rise in response to a blockage in the liver.

Given the biology, ALT and AST are likely to rise early
on in any drug induced liver damage and initial focus should
be on those.
ALT and AST are closely correlated in this example and in
what follows we focus mostly on ALT.
%
\section{Elimination of baseline effect\label{sec:baseline}}
%
Since the four lab variables were measured at baseline and
at a later date in the same patients, we might reasonably
expect the two values to be related. Figure~\ref{fig:scatterplots} scatterplots (on
the log scale) reveal this to be true.
\begin{figure}
\begin{center}
<<xyplot, fig=TRUE>>=
print(
xyplot(ALT.M ~ ALT.B | dose, data=liver,
      as.table = TRUE,
      scales=list(log=2))
)
@
\end{center}
\caption{Scatterplots of post-baseline versus baseline
ALT.\label{fig:scatterplots}}
\end{figure}

We can eliminate the
baseline effect (and therefore reduce the variance) using
a linear model. Since we have no reason to suppose the
data to be normally distributed, and since Figure~\ref{fig:scatterplots}
reveals outliers not consistent with a Gaussian distribution,
we will use a robust regression approach. (In general, lab
safety data should never be assumed to be noramlly distributed.)

Following \cite{maronnaMartinYohai} (page 144), we prefer
an MM-estimated regression line with Gaussian efficiency
set to 85\% and bisquare weight functions.
This model can be estimated using the
{\tt rlm} function in the {\tt MASS} package. By default,
{\tt rlm} uses 95\% Gaussian efficiency, and we can
obtain the required 85\% by passing argument {\tt c = 3.44}
(see \cite{maronnaMartinYohai}, page 30) into the
function.

<<MM, fig=TRUE>>=
rmod <- rlm(log(ALT.M) ~ log(ALT.B) + ndose, data=liver, c=3.44, method="MM")
summary(rmod)
par(mfrow=c(2, 2))
plot(fitted(rmod), resid(rmod), xlab="Fitted", ylab="Residuals")
abline(h=0, col=2)
qqnorm(resid(rmod))
qqline(resid(rmod))
plot(log(liver$ALT.M), fitted(rmod), xlab="Observed", ylab="Fitted")
d <- density(resid(rmod))
hist(resid(rmod), xlab="Residuals", prob=TRUE, ylim=range(d$y))
lines(d)
rug(resid(rmod))
@

The residual plots do not give cause for concern and reveal
several outliers, justifying the use of a robust method of
estimation.

In practice, it would be sensible to fit a few models,
possibly using {\tt dose} as a factor and with alternative
transformations of ALT. That part of the analysis is left
as an exercise and we proceed on the understanding
that the fitted robust linear model is adequate for eliminating
the effects of baseline and dose on the central tendency of the
data.

We can now obtain the residuals to be used for the
extreme value modelling of ALT. We also plot scaled residuals to get
a preliminary impression of any dose effect.

<<resids, fig=TRUE>>=
liver$r <- resid(rmod)
par(mfrow=c(1, 1)) # Reset graphics device
 # Scaled residuals
plot(jitter(liver$ndose), liver$r / rmod$s,
    xlab="Dose", ylab="Scaled residuals",
    axes=FALSE)
box(); axis(2)
axis(1, at=1:4, labels=LETTERS[1:4])
abline(h=c(-2, 0, 2), lty=c(2, 1, 2), col=c(3,4,3))
@

It can be seen that there are several large outliers,
out of keeping with any assumptions about normality, and
that dose D appears to be associated with more, and larger,
outliers.
%****************************************************************
%****************************************************************
%
\section{Generalized Pareto distribution models for ALT}
%
We now proceed to fit, evaluate, choose between, and ultimately
make predictions from generalized Pareto distribution (GPD) models for the residuals
of the ALT data obtained in Section~\ref{sec:baseline}.
%
\subsection{Extreme value modelling and asymptotic motivation for the GPD}
%
Extreme value statistical models are unusual among statistical models in that they are often required for extrapolation beyond levels observed in the data.  As statisticians, we are told that extrapolation from statistical models is perilous: our models can only be trusted in regions where we have sufficient data to calibrate and check goodness of model fit.  Extreme value modelling has responded to a demand for extrapolation beyond this safe region.  Since we can no longer rely on data as a check on our model's suitability, extreme value statisticians turn to mathematical arguments to bolster their confidence in their extrapolation.  These arguments provide a justification for the use of a particular type of model to describe tail behaviour of random variables.

This is not a tutorial in Extreme Value Theory, for which we refer the reader to \cite{coles}, which describes a range of methods for modelling the statistical properties of sample maxima, threshold excesses, extremes of dependent series and other aspects of tail behaviour.  

The {\tt texmex} package focusses on the use of {\it threshold exceedances}. Specifically, we fit the generalised Pareto Distribution,
GPD($\sigma, \xi$)~\cite{davisonSmith} to 
data points in excess of suitably chosen thresholds.
The GPD has distribution function
\begin{eqnarray}
\label{eqn:genParetoDistnFun}
F_{>u}(x)=1-\left\{1+\xi\left(\frac{x-u}{\sigma}\right)\right\}^{-1/\xi}
\mbox{ for }x>u,
\end{eqnarray}
where $u$ is the threshold for fitting and $\sigma>0$ and $\xi \in
\IR$ are the scale and shape parameters respectively.  This is the
conditional distributon of observations given that the observations
exceed the fitting threshold $u$. The range of possible values taken
by realisations from the GPD depends on the parameter values, with the
distribution having a finite upper end point (short tailed) if the
shape parameter is negative ($u<x\leq u-\sigma/\xi$ if $\xi<0$) and an
infinite tail otherwise $u<x<\infty$ if $\xi\geq 0$.  When $\xi=0$, the GPD corresponds exactly to the Exponential distribution.

Extreme value theory tells us that under appropriate normalisation of
the threshold excesses, as the threshold $u$ tends to the
distributional upper endpoint, the limiting distribution of the
excesses must fall in the generalised Pareto family of distributions
(given certain conditions concerning non-degeneracy of the limit
distribution and smoothness of the distribution of the original
variable). So whatever the original distribution of the measurements,
provided we choose an appropriately high threshold, the distribution
of values exceeding that threshold should be well approximated by a
GPD.  Diagnostic tools to aid the choice of suitable threshold are
standard, and are described shortly -- see also~\cite{coles}.
%
\subsection{Parameterization}
%
The usual parameterization of the GPD (as in Equation~(\ref{eqn:genParetoDistnFun})) is in terms of its
scale paramter $\sigma$ and shape parameter $\xi$. There
are, however, good reasons for reparameterizing in terms
of $\phi = \log\sigma$:
\begin{itemize}
\item Experience has demonstrated that the numerical
algorithms used for optimizing the log-likelihood tend
to converge more reliably when working with $\phi$;
\item When including covariates in the model we are
faced with the constraint that $\sigma > 0$ and working
with a linear predictor specified in terms of $\phi = \log\sigma$
guarantees this constraint;
\item When placing prior distributions on parameters,
it is convenient to work with Gaussian distributions and
$\phi$ is more likely to be close to Gaussian than is
$\sigma$.
\end{itemize}
As such, some of the functions in {\tt texmex} work with $\phi$, not
$\sigma$. In the case when inference is required for $\sigma$ rather than $\phi$, the point estimates
can simply be exponentiated if
maximum likelihood estimation is used. If a prior distribution is
used, the point estimates are not invariant to transformation,
so any transformed values should only be considered to
be approximate.
%
\subsection{Threshold selection}
%
GPD modelling proceeds by selecting a threshold above
which the data appear to be well modelled. Standard
tools for threshold selection that appear in the
literature (see for example \cite{coles}) the \emph{mean residual
life} plot, and plots of parameters estimated using a range of
thresholds, \emph{threshold statbility plots}.

For a suitably chosen threshold, the mean residual life
plot should be linear and the parameter estimates in threshold stability plots 
constant beyond the threshold (up to sampling variation). The slope of the linear part of the MRL plot also gives an indication of the sign of the shape parameter and the shape of the tail -- negative slope shows a short tailed distribution, a horizontal line (zero gradient) shows an exponential type tail and a positive slope suggests a heavy tailed distribution.

<<threshold, fig=TRUE>>=
par(mfrow=c(2, 2))
gpdRangeFit(liver$r, main=c("Range fit: scale","Range fit:shape"))
mrlPlot(liver$r, main="MRL plot")
@


For our example, when fitting the GPD to the residuals from our robust regression, a threshold slightly above 0 appears to be sensible. However, we will need to
do some additional diagnostics to check this.
We proceed by selecting the $70^{th}$ percentile
of the residuals as being the candidate threshold.

In many examples, we have found that the $70^{th}$ or quite
often the $50^{th}$ percentile is a suitable threshold.
The theory underpinning the GPD tells us that there has to be a
threshold above which the GPD fits the data, but the
theory does not specify that the threshold needs to be
high. In our case, we are modelling residuals, and so the
complete distribution will be near-symmetric and the GPD
will only fit one tail. So it is unreasable to use
thresholds lower than about the $50^{th}$ percentile.
%
\subsection{Maximum penalized likelihood estimation}
%
With small sample sizes, the GPD log-likelihood function
often becomes flat and parameter estimates fail to converge.
One way to overcome this is to penalize the likelihood by
some function of the parameters. Experience suggests that
the main problems are overcome by putting fairly modest
penalties on $\xi$.

Thus, rather than maximize the log-likelihood $l(\phi, \xi | X)$
we maximize
\begin{eqnarray}
l(\phi, \xi) - \lambda \xi^2 \label{eqn:MPL}
\end{eqnarray}
for some $\lambda$.

\subsubsection{Choice of $\lambda$}
If we exponentiate (\ref{eqn:MPL}), the result can be
written as
\begin{eqnarray}
L(\phi, \xi | X) e^{-\xi^2/2\theta^2} \label{eqn:LP}
\end{eqnarray}
in which $\theta = \sqrt{\frac{1}{2\lambda}}$. The rightmost
term in (\ref{eqn:LP}) is proportional to a Gaussian
distribution centred at 0. Thus, maximum penalized
likelihood estimation has a Bayesian interpretation
and corresponds to maximum a posteriori estimation.

For the GPD, $\xi = -1$ corresponds to the distribution
being uniform, $\xi = 0$ corresponds to it being
exponential, and $\xi = 1$ corresponds to it being so
heavy-tailed that its expectation is infinite. For the
kinds of data we have, $\xi = -1$ and $\xi = 1$ are
implausible, and we would expect values of $\xi$ to be
fairly close to 0. This implies a prior distribution
that is Gaussian with standard deviation $\theta = \frac{1}{2}$.

Since convergence issues are generally associated with
$\xi$, we can choose a diffuse prior for $\phi$,
$\phi \sim N(0, 10^4)$, say.

In general, we will attempt to use MLE or the use of
diffuse priors for both $\phi$ and $\xi$.
Prior distribution $\xi \sim N(0, \frac{1}{4})$
independently of a diffuse prior on $\phi$ can be 
used when convergence issues arise. Such a model can
be fit using

<<MPLE, eval=FALSE>>=
pp <- list(c(0, 0), diag(c(10^4, .25)))
pmod <- gpd(x, qu=.7, priorParameters = pp, prior="gaussian")
@

in which {\tt priorParameters} contains the mean $(0, 0)^T$
and covariance matrix of the prior Gaussian distribution.
%
\subsection{Model selection}
%
We can fit GPD models with covariates in $\phi$,
in $\xi$, in neither, or in both. We will first
fit a simple model with no covariates and examine
some diagnostic plots to see if there are any
obvious problems suggesting that a higher threshold
needs to be used.

<<simplegpd, fig=TRUE>>=
mod <- gpd(r, data=liver, qu=.7, penalty="none")
mod
par(mfrow=c(2, 2))
plot(mod)
@

The shaded regions in the p-p and q-q plots indicate pointwise 95\% tolerance
intervals, based on 1000 simulated datasets. The shaded region in the return level plot shows 95\% pointwise confidence intervals. The plots
show no systematic departure of the data from the model at this choice of threshold, so
we proceed to fit various models with covariates and
compare them using AIC.

<<gpdmods1>>==
mod1 <- gpd(r, data=liver, qu=.7, penalty="none", phi= ~dose, xi= ~dose)
mod2 <- gpd(r, data=liver, qu=.7, penalty="none", phi= ~ndose, xi= ~ndose)
AIC(mod1)
AIC(mod2)
@
AIC is lower for {\tt mod2} which treats {\tt dose} as \ numeric variable rather than as a factor at 4 levels.  So we prefer {\tt mod2}.  We proceed to try simplifications of this model:
<<gpdmods2>>==
mod3 <- gpd(r, data=liver, qu=.7, penalty="none", phi= ~ndose)
mod4 <- gpd(r, data=liver, qu=.7, penalty="none", xi= ~ndose)
AIC(mod3); AIC(mod4); AIC(mod)
@

Since {\tt mod4}
has the lowest AIC we prefer that model. This is the model with $\phi$ constant and $\xi$ linear in dose (on the log scale).  In practice, a few more models
might have been tried. Again, that part of the analysis is left as
an exercise.

We now take a closer look at {\tt mod4}.

<<mod4diag, fig=TRUE>>=
par(mfrow=c(2, 2), pty="s")
plot(mod4)
summary(mod4)
@

Since there is a covariate in the model the probability and quantile plots are constructed using the model residuals, which are exponential under the fitted model. We also have a plot of the residuals against the fitted parameters for any parameter that is modelled using a covariate (in this case the shape parameter $\xi$). A well fitting model should have homogeneity of residuals across different values of the fitted parameter.  These diagnostic plots give no cause for concern. 

From the estimated parameters, we see that as dose increases, so does
the estimate of $\xi$, starting out negative for dose A (suggesting
a fairly short-tailed distribution), but moving to positive at
the higher doses (suggesting a heavy-tailed distribution).
%
\subsection{Predicted return levels}
\label{subsect:retLevelsPtEsts}
%
We can now combine our expected values from the robust linear regression
and our GPD parameter estimates to get return levels. 

The general definition of an $m$-observation return level for the GPD is:
\begin{eqnarray}
x.M = u + \frac{\sigma}{\xi}\{(mp)^\xi -1 \}.\label{eqn:returnLevel}
\end{eqnarray}
Here $p$ is the probability of exceeding the GPD fitting threshold
$u$ and $m$ is a large value, so that $x.M$ is termed the
{\emph m-observation return level} and represents the maximum
value of $x$ expected to be seen in $m$ observations at a give dose.

For our application to the {\tt liver} dataset, prediction of $m$-observation ($m$-patient) return levels is complicated by the relationship between baseline and post-treatment response which must be accounted for.  In order to do this, we must specify a value of baseline and then the value of $u$ in Equation~(\ref{eqn:returnLevel}) is calculated as the expected post-treatment value given that baseline value, plus the threshold used for fitting the GPD model to the residuals.  The resulting value of $x.M$  is then interpreted as the {\emph m-patient return level for patients with that given baseline value}.

So for our fitted model {\tt mod4}, for a given baseline,  $u$ in equation~(\ref{eqn:returnLevel}) is calculated as the expected post-treatment value given that baseline plus the $70^{th}$ percentile
of the residuals.  The $m$-patient return level depends on \emph{dose} via the threshold (since the expected values from the regression depend on dose) and through $\xi$ which is also a function of dose.

The estimated robust regression and GPD models are combined in R to obtain return level estimates as follows:
<<returnLevelsPointEsts>>=
calcRetLevel <- function(gpdModel,MMmodel, m,base){ 
  coGPD <- coef(gpdModel)
  phi <- coGPD[1]
  xiA <- coGPD[2] + coGPD[3]
  xiB <- coGPD[2] + coGPD[3]*2
  xiC <- coGPD[2] + coGPD[3]*3
  xiD <- coGPD[2] + coGPD[3]*4

  eLogALT <- predict(MMmodel,newdata=data.frame(ALT.B=rep(base,4),ndose=1:4))

  rl <- function(u, phi, xi, p, m){
      u + exp(phi) / xi * ((m * p)^xi - 1)
  }
  u <- eLogALT + gpdModel$threshold
  rl <- rl(u, phi, c(xiA,xiB,xiC,xiD), gpdModel$rate,m=m)

  names(rl) <- names(u) <- LETTERS[1:4]
  list(u=exp(u),RL=exp(rl))
}
@

An illustration of this calculation is given in Figures~\ref{fig:pointEstRetLevelMeanBaseline} and~\ref{fig:pointEstRetLevelLowerUpperQuartileBaseline}.
First we carry out the calculation for patients with baseline equal to the mean observed baseline in the {\tt liver} dataset.

<<returnLevelsPointEstsPlotCalc>>=
PlotMpatientRetLevel <- function(Baseline,ylim,Legend=TRUE){

  m <- exp(seq(log(10),log(1000),length=20))
  RL <- t(sapply(m,function(x,gpd,rmod,base){
                     calcRetLevel(gpd,rmod,x,base)$RL},gpd=mod4,rmod=rmod,base=Baseline))
  u <- calcRetLevel(mod4,rmod,m[1], Baseline)$u

  coGPD <- coef(mod4)
  plot(rep(c(10,m),4),rbind(u,RL),type="n",xlab="m",
       ylab="m-patient return level",log="x",
       main=paste("ALT: Baseline =",signif(Baseline,3)),ylim=ylim)
  for(i in 1:4){
    lines(m,RL[,i],col=i)
    abline(h=u[i],col=i,lty=2)
  }
  if(Legend){
    legend(min(m),ylim[2],legend=c(paste(LETTERS[1:4],", xi =",
                                         signif(c(coGPD[2] + coGPD[3]*(1:4)),3)),
                                   paste("U,",LETTERS[1:4])),
           col=rep(1:4,2),lty=rep(1:2,each=4))
  }
}
@
\begin{figure}
\begin{center}
<<returnLevelsPointEstsPlot, fig=TRUE>>=
PlotMpatientRetLevel(mean(liver$ALT.B),ylim=c(0,200))
@
\end{center}
\caption{Point estimates of $m$-patient return levels under each treatment, for patients with baseline equal to the mean  observed baseline.\label{fig:pointEstRetLevelMeanBaseline}}
\end{figure}

Figure~\ref{fig:pointEstRetLevelMeanBaseline} shows estimated $m$-patient return levels for a patient with baseline equal to the mean observed value of baseline.  For such patients, differences in the values of the thresholds $U$ are due to the difference in expected post-treatment ALT estimated by the MM regression model, whose coefficients depend on treatment (dose).

For small values of $m$, estimated $m$-patient return levels in Figure~\ref{fig:pointEstRetLevelMeanBaseline} show little additional difference between treaments above that already due to the diffences in mean post-treatment ALT captured by the robust regression model.  However, as $m$ increases and we extrapolate further into the tail of the the distribution, differences in shape parameter $\xi$ between the different doses have greater influence on the estimated return levels.  Those treatments with heavier tails (treatments C and D -- the higher doses) have return level estimates that grow much faster than those corresponding to treatments with short tails (treatments A and B -- the lower doses).

Figure~\ref{fig:pointEstRetLevelLowerUpperQuartileBaseline} shows correponding return level estimates for patients with baseline equal to the lower and upper quartiles of the observed baseline.  The absolute value of $m$-patient return level depends on the baseline, but the ordering between the four treatments due to the fitted GPD shape parameters is the same in each case.
\begin{figure}
\begin{center}
<<returnLevelsPointEsts2, fig=TRUE>>=
par(mfrow=c(1,2),pty="s")
PlotMpatientRetLevel(quantile(liver$ALT.B,0.25),ylim=c(0,200),Legend=FALSE)
PlotMpatientRetLevel(quantile(liver$ALT.B,0.75),ylim=c(0,200),Legend=FALSE)
@
\end{center}
\caption{Point estimates of $m$-patient return levels under each treatment, for patients with baseline equal to the lower (left hand plot) and upper quartiles of observed baseline values.  For plot legend, refer to Figure~\ref{fig:pointEstRetLevelMeanBaseline}.
\label{fig:pointEstRetLevelLowerUpperQuartileBaseline}}
\end{figure}
%
\subsection{Uncertainty estimation}
%
We would like to gain an understanding of the uncertainty associated with our estimated models and with the quantities of interest derived from them.  This will include variation due to parameter estimation.  Figure~\ref{fig:pointEstRetLevelLowerUpperQuartileBaseline}) illustrates how predictions can depend on patient baseline, which is inherently variable, so we also need to account for this source of uncertainty.  Hence we require to combine different sources of uncertainty in our final reckoning. This is achieved by adopting a simulation based approach.  The methods of simulation used to represent each source of uncertainy are described now.
%
\subsubsection{GPD model parameter uncertainty}
%
Standard methods which assume normality of MLEs, with standard errors well approximated by estimates derived from the Information Matrix are often inappropriate summaries of GPD model parameter and return level estimate uncertainty.  This is due to the asymmetry of the log-likelihood function about the MLE, particularly for the shape parameter.  A more accurate description of uncertainty can be achieved by the adoption of a Bayesian approach, which simulates from the posterior distributions of the parameters. This is carried out in {\tt texmex} by using the {\tt gpd} function again, this time using {\tt method = "simulate"} to tell the function to simulate from the joint posterior distribution of the parameters. 

<<gpdmcmc, eval=FALSE>>=
bmod <- gpd(r, data=liver, qu=.7, xi= ~ndose, method="simulate")
@

Equivalently, the Bayesian estimation based on MCMC can also be instigated by the use of the function {\tt update} on the previously chosen model.  The method of estimation is changed from {\tt "optimize"} -- under which estimation is carried out using (penalized) maximum likelihood -- to {\tt "simulate"} -- under which a Metropolis algorithm is used to simulate from the joint posterior distribution of the parameters.  For our preferred model, {\tt mod4}, this is implemented as follows:

<<update,fig=TRUE, eval=TRUE>>=
bmod <- update(mod4,method="simulate",trace=10000,penalty="gaussian")
par(mfrow=c(3, 3))
plot(bmod)
summary(bmod)
@

The plots of the Markov chains ought to look like ``fat hairy 
caterpillars'' if the algorithm has converged on its target
distribution. Also, the cumulative means of the chains should
converge, the acceptance rate should not be too high or too low, and the
autocorrelation functions should rapidly decay to zero. We 
conclude from the plots that there is no evidence against convergence of our Markov chains, although we should probably thin our output to obtain a chain that is closer to independent.  Here we retain the burn-in of 500 but now discard all but every 5th observation, resulting in an autocorrelation function which decays more rapidly to zero.

<<thinmcmc, fig=TRUE>>=
bmod <- thinAndBurn(bmod,burn=500, thin = 5)
par(mfrow=c(3, 3))
plot(bmod)
summary(bmod)
@

Since $\phi$ does not vary by {\tt dose}, we can use the simulated
values of $\phi$ to obtain predictions for any dose.
However, $\xi$ varies by dose, so we need to make sure
we use the correct value for each tgreatment. The simulated parameters
are found in {\tt bmod\$param}.

<<simparams>>=
phi <- bmod$param[, 1]
xiA <- bmod$param[, 2] + bmod$param[, 3]
xiB <- bmod$param[, 2] + bmod$param[, 3] * 2
xiC <- bmod$param[, 2] + bmod$param[, 3] * 3
xiD <- bmod$param[, 2] + bmod$param[, 3] * 4
nsim <- length(phi)
@
Since the Markov chains from the posterior distributions
of the parameters in the GPD model were run for 10000
steps (having dropped 500 as burn-in), and then thinned to a remaining 2000 values, the value of {\tt nsim} in the above is
<<nsim>>=
nsim
@
%
\subsubsection{Uncertainty due to variation in baseline}
%

We simulate baseline values simply by resampling from the
observed values. The regression model was fit to the logs
of the data, so we resample the logs.  We simulate {\tt nsim} independent values for each of the four treatment levels:

<<simbase>>=
base <- sample(log(liver$ALT.B), size=4*nsim, replace=TRUE)
@

%
\subsubsection{Robust regression model parameter uncertainty}
%
We now need to simulate regression coefficients. MM-estimates
are asymptotically Gaussian distributed, so we simulate from
their joint Gaussian distribution. First we need to construct
the covariance of the regression parameters from the values returned by the {\tt rlm} summary
function.

<<simreg>>=
mycov <- summary(rmod)$cov.unscaled * summary(rmod)$stddev^2
myloc <- coef(rmod)
mycoefs <- rmvnorm(4*nsim, mean=myloc, sigma=mycov)
@
%
\subsection{Return level estimation uncertainty}
%
We now combine all the simulations from the different components of our model to obtain an estimate of uncertainty about our point estimates of $m$-patient return levels.  Whereas before (Section~\ref{subsect:retLevelsPtEsts}) we had to fix a value of baseline, we can now average over baseline values within our sample.

Simulated expected post-treatment ALT are obtained by combining the simulated baseline values with the simulated robust regression coefficients.

<<simexp>>=
eALT <- mycoefs[, 1] + mycoefs[, 2] * base + rep(1:4, each=nsim) * mycoefs[, 3]
@

Note that the variability in the estimated regression parameters
will be drawfed by the variability due to extrapolation
from the GPD model and by that due to variability in baseline values. As an exercise, try fixing the robust regression
coefficients at their point estimates and see how much difference
it makes to the final predictions.

The expected post-treatment values of ALT are combined with the simulated  GPD parameters to obtain simulated values of $m$-patient return levels (Equation~\ref{eqn:returnLevel}) on the log scale:

<<returnLevelsInts>>=
rl <- function(u, phi, xi, p, m=1000){
    u + exp(phi) / xi * ((m * p)^xi - 1)
}
u <- eALT + quantile(liver$r, .7)
rlA <- rl(u[1:nsim], phi, xiA, .3)
rlB <- rl(u[(nsim+1):(2*nsim)], phi, xiB, .3)
rlC <- rl(u[(2*nsim + 1):(3*nsim)], phi, xiC, .3)
rlD <- rl(u[(3*nsim +1):(4*nsim)], phi, xiD, .3)
@

Finally,
we exponentiate to get back to the scale of the raw data.  Point estimates and approximate 90\% credible intervals are estimated respectively by the empirical medians and 5 and 95\% quantiles of our simulated return levels.

<<returnLevelsIntsPLot,fig=TRUE>>=
srl <- rbind(quantile(rlA, prob=c(.05, .5, .95)),
             quantile(rlB, prob=c(.05, .5, .95)),
             quantile(rlC, prob=c(.05, .5, .95)),
             quantile(rlD, prob=c(.05, .5, .95)))
srl <- exp(srl)

plot(srl[, 2], 1:4, xlim=range(srl), type="n", log="x",
     axes=FALSE, xlab="1000-patient return level\n(90% interval)", ylab="Dose")
points(srl[, 2], 1:4, pch=16, cex=1.25)
segments(x0=srl[, 1], x1=srl[, 3], y0=1:4)
box()
axis(1)
axis(2, at=1:4, labels=LETTERS[1:4])
@

From the resulting figure, there is some evidence that 
dose D is associated with very large values of ALT, suggesting
a potential safety issue.

%
\subsection{Probability estimation uncertainty}
%
We can also predict quantities such as $P(ALT > 3\times ULN)$
by rearranging (\ref{eqn:returnLevel}):
\begin{eqnarray*}
\frac{1}{m} = p\left[(x.M - u)\frac{\xi}{\sigma} + 1\right]^{-1/\xi}.
\end{eqnarray*}

Again, things are made complicated by the need to add
the baseline effect back into the predictions. Here, we need
also to account for the fact that for some fairly low values
of interest (e.g.\ ULN, the Upper Limit of Normal), it is possible for the expected value
of ALT given baseline to be greater than the specified value (in cases where baseline ALT is elevated). If
the expected value of ALT is greater than ULN, we need to consider
the full distribution function $F(r)=P(R\leq r)$ of the residuals. The model for this distribution has two components: above the threshold for fitting, the excesses of the residuals over the threshold are modelled by the fitted GPD; below the threshold we use the empirical distribution $\tilde F(r)$:
\begin{eqnarray*}
\hat F(r) = \left\{ \begin{array}{ll} GPD(r-u; \sigma, \xi) & r > u\\
                                      \tilde F(r) & r \leq u.
                    \end{array} \right.
\end{eqnarray*}

This consideration is irrelevant for higher values of interest above which
the expected value of ALT will never fall (e.g.\ interest is more usually in values $3\times ULN$ and
almost always $10\times ULN$ or higher).

Noting that in the {\tt liver} dataset, the ULN for ALT was 36 U/L,
the code below does the necessary computations.

<<predprobs>>=
rp <- function(xm, u, phi, xi, p, r) {
        res <- p * (1 + xi/exp(phi) * (xm - u))^(-1/xi)

        # Need to adjust for cases in which u > xm
        # The distribution is a the semi-parametric empirical / GPD distribution 
        # F(resid)_(resid<u) + P(resid < xm -u)_(resid > u) as the answer we want
        if (any(u > xm)){
                ur <- u[u > xm]
                rr <- r[r < quantile(r, 1-p)]
                mm <- sapply( ur, function(x, r, m, p){mean(r < m - x)},
                              r=rr, m=xm, p=p)
                res[u > xm] <- p + (1-p)*mm
        }
        res[xi < 0 & xm > u - exp(phi)/xi] <- 0
        res
}
getProbs <- function(u, phi, xi, p, r, ULN, m = c(1, 3, 10, 20)) {
        m <- log(ULN * m)
        res <- t(sapply(m, rp, u = u, phi = phi, xi = xi, p = p, r=r))
        res <- apply(res, 1, function(x){ c(quantile(x, c(.05, .95)),
                     Mean=mean(x))[c(1,3,2)] })
        round(res, 4)
}

rpA <- getProbs(u = eALT, phi = phi, xi = xiA, r=liver$r, p = 1-0.7,ULN = 36)
rpB <- getProbs(u = eALT, phi = phi, xi = xiB, r=liver$r, p = 1-0.7,ULN = 36)
rpC <- getProbs(u = eALT, phi = phi, xi = xiC, r=liver$r, p = 1-0.7,ULN = 36)
rpD <- getProbs(u = eALT, phi = phi, xi = xiD, r=liver$r, p = 1-0.7,ULN = 36)
cnames <- paste("P(ALT > ", c("", "3x", "10x", "20x"), "ULN)",sep = "")
colnames(rpA) <- colnames(rpB) <- colnames(rpC) <- colnames(rpD) <- cnames
rpA
rpB
rpC
rpD
@

The above tables contain summary statistics from posterior
predictive distributions. When computing the return levels, we
used the median as an estimate of the centre of the posterior
distribution. The reason for this is that experience has shown
that for some distributions with extremely heavy tails, the mean
can be unduly influenced by very large values and can even be
outside the 90 or 95\% postserior interval (as estimated by the
$5^{th}$ and $95^{th}$ percentiles). The distribution is often skewed,
so the mean and median can differ substantially; in an ideal
world we might display the entire posterior rather than simple
summaries.  For the estimation of probailities, the range of 
values is bounded by $[0,1]$ and so here we report sample means instead of medians.

The tables suggest that under dose D (the highest dose), approximately 0.2\% of patients
(about 1 in 500) will get an ALT greater than $20\times ULN$, 
a level described by CTC as {\em life threatening}.
%***************************************************************
%***************************************************************
%
\section{Multivariate extremes}
%
Modelling of multivariate extreme values is more complicated
than univariate modelling. An issue that quickly arises is how 
to define a multivariate extreme observation. If an
observation has to be extreme in all components simultaneously,
the amount of data to model quickly diminishes to numbers too
small to do anything meaningful with. Moreover, dependencies
between variables in the body of the data do not necessarily
tell us anything at all about dependence in the extremes.

Heffernan and Tawn~\cite{heffernanTawn} proposed a conditional
approach to multivariate extreme value modelling that we will
pursue here. Firstly, however, we attempt to get a feel for the
data by examining the extremal dependence of some pairs of
variables.

Multivariate approaches to extreme value modelling are less
developed than univariate approaches and it is difficult to
include covariates in the analysis explicitly. Henceforth,
we treat the four dose groups independently of one another.
To reduce the amount of code and output, we restrict attention
to the lowest and highest doses, i.e. doses A and D, and we
ignore the AST variable which mimics closely the behaviour of the ALT variable.

We first set up our datasets.

<<mvdata>>=
getData <- function(data){
    ralt <- resid(rlm(log(ALT.M) ~ log(ALT.B), data=data, method="MM", c=3.44))
    rast <- resid(rlm(log(AST.M) ~ log(AST.B), data=data, method="MM", c=3.44))
    ralp <- resid(rlm(log(ALP.M) ~ log(ALP.B), data=data, method="MM", c=3.44))
    rtbl <- resid(rlm(log(TBL.M) ~ log(TBL.B), data=data, method="MM", c=3.44))
    data.frame(ralt=ralt, rast=rast, ralp=ralp, rtbl=rtbl)
}
Adat <- getData(liver[liver$dose == "A",])
Bdat <- getData(liver[liver$dose == "B",])
Cdat <- getData(liver[liver$dose == "C",])
Ddat <- getData(liver[liver$dose == "D",])
@

In practice, we would have performed some diagnostic checks on the
robust regression models. It is sometimes the case that square-root
transformations are better suited than log transformations, particuarly
for TBL.

We plot the data in Figure~\ref{fig:alt.alpData} -- these are the residuals from the robust regression modelling.

\begin{figure}
\begin{center}
<<mvdataplots1, fig=TRUE>>=
par(mfrow=c(2,2))
plot(Adat$ralt,Adat$ralp,xlab="ALT",ylab="ALP",main="Residuals, dose A")
plot(Bdat$ralt,Bdat$ralp,xlab="ALT",ylab="ALP",main="Residuals, dose B")
plot(Cdat$ralt,Cdat$ralp,xlab="ALT",ylab="ALP",main="Residuals, dose C")
plot(Ddat$ralt,Ddat$ralp,xlab="ALT",ylab="ALP",main="Residuals, dose D")
@
\end{center}
\caption{Scatterplots of residuals from robust fitting, by dose group. \label{fig:alt.alpData}}
\end{figure}

We see that the dependence between these residuals appears to increase as the dose is increased. Plotting the other pairs of variables is left as an exercise.
%
\subsection{Exploring pairwise extremal dependence}
%
We can examine pairwise extremal dependence by plotting summary
statistics $\chi$ and $\bar\chi$ as defined by Coles, Heffernan
and Tawn~\cite{colesHeffernanTawn}. We do so only to look at
associations between ALT and TBL, and between ALT and ALP, for
dose D only.

<<chi, fig=TRUE>>=
DchiTBL <- chi(Ddat[, c("ralt", "rtbl")])
DchiALP <- chi(Ddat[, c("ralt", "ralp")])
par(mfrow=c(2, 2))
plot(DchiTBL, main1="Chi: ALT + TBL", main2="Chi-bar: ALT + TBL")
plot(DchiALP, main1="Chi: ALT + ALP", main2="Chi-bar: ALT + ALP")
@

The plots are interpreted as follows:
\begin{description}
\item[a. Look at limiting value of $\bar\chi(u)$ \bf plot as the quantile $u$
    tends to 1].  This gives a diagnostic as to whether the data
  exhibit asymptotic dependence (the very largest values of each variable tend to occur in the same observation).  A limiting value of 1 is indicative
  of asymptotic dependence.
\item[b. If limit in a. is equal to 1] examine plot of
  $\chi(u)$ for a measure of the strength of dependence within the
  asymptotic dependence class.  The limiting value of this function as
  the quantile $u\rightarrow1$ tells us about the strength of this dependence, with
  values closer to 1 indicating stronger dependence.
\item[c. If limit in a. is less than 1] examine plot of
  $\bar\chi(u)$ for a measure of the strength of dependence within the
  asymptotic independence class. Although at asymptotic levels, the largest values of the variables tend not to occur in the same observation, at moderately extreme levels, dependence may still be relatively strong.  The limiting value of this function as
  $u\rightarrow1$ tells us about the strength of this dependence, with
  values closer to 1 indicating stronger dependence.
\end{description}
The $\bar\chi$ for ALT and TBL shows that these variables are likely to be asymptotically independent, with weak dependence within this class. For ALT and ALP, both $\bar\chi$ and $\chi$ plots rise towards the right, and include 1 as a possble limit
indicating possible asymptotic dependence with strong extremal dependence within this class.

An alternative approach to examining pairwise extremal dependence
is to examine the multivariate conditional Spearman's correlation
coefficient across a 
sliding window of values of the variables, following Schmidt
and Schmitt~\cite{schmidtSchmitt}.

<<mcs, fig=TRUE>>=
DmcsTBL <- MCS(Ddat[, c("ralt", "rtbl")])
DmcsALP <- MCS(Ddat[, c("ralt", "ralp")])
par(mfrow=c(1, 2))
plot(DmcsTBL, main="MCS: ALT + TBL")
plot(DmcsALP, main="MCS: ALT + ALP")
@

The plots of the multivariate conditional Spearman's $\rho$
do not have the same vertical axes, and tell a similar story
to the plots of $\chi$ and $\bar\chi$. Had we wanted to put
confidence intervals on the MCS plots, we could have used
{\tt bootMCS} and its associated plot function (left as
an exercise).

The exploratory summaries of this section suggest that when we come
to the conditional multivariate extreme value modelling, we should
expect to find an association between extreme ALT and extreme
ALP, but not between ALT and TBL, for dose D. The reader is
left to check the other doses to see if these findings are 
similar across the dose range.
%
\subsection{Conditional multivariate extreme value modelling**}
%
** This section is of a technical nature and may be skipped over at first reading.\\~~

The conditional multivariate approach of Heffernan and Tawn
proceeds by first fitting GPD models to the marginal variables,
then estimating the dependence structure. Like the GPD model for excesses above a threshold, the dependence component of the Heffernan and Tawn model also conditions on a variable exceeding a threshold.  It then seeks to describe the conditional distribution of the remaining variables given the threshold excess by the first variable, using a regression type model.  Uncertainty in the
parameters in the dependence structure can be characterized via
a bootstrap scheme.
%
\subsubsection{Marginal transformation}
%
The structure of the regression type dependence model is defined not on the original data scale, but after marginal transformation to standardised margins.  In the original implementation, Heffernan and Tawn used a transformation to Gumbel margins but subsequent developments in this area show the structure of the regression model to be greatly simplified if Laplace margins are used instead.  The package {\tt texmex} implements both and correspondingly we describe both here.  Let $\bX = (X_1,\ldots,X_d)$ be a $d$ dimensional random variable with arbitrary marginal distributions.  Let $\hat F_i$ denote an estimate of the $i$th marginal distribution function ($i=1,\ldots,d$) and let $G$ denote the distribution function of the standardised marginal distribution, to be determined.  The original vector variable $\bX$ is transfromed to $\bY=(Y_1,\ldots,Y_d)$, a variable having standardised marginal distributions by using the \emph{probability integral transform} as follows:
\begin{eqnarray}
\label{eqn:PIT}
  Y_i &=& (G^{-1}(\hat F_i(X_i)), i=1,\ldots,d.
\end{eqnarray}
In practice, the $\hat F_i$ can be the marginal empirical distribution functions of the data (in which case Equation~(\ref{eqn:PIT}) is also known as the \emph{rank transform}, or the semi-parametric model using the empirical distributions below a threshold and the fitted GPD models for the tails of the distributions above the threshold.
%
\subsubsection{Regression model structure}
%
Let $Y_i, i \epsilon \{1,\ldots,d\},$ be the variable on which we are to condition.  Then $\bY_{-i}$ denotes the remainder of the vector $\bY$ excluding the $i$th component. The Heffernan and Tawn approach conditions on $Y_i$ being above some high threshold $t$, and models the dependence of the remaining $\bY_{-i}$ conditional on the value of $Y_i>t$.  The form of the regression model for the conditional dependence structure depends on the precise choice of $G$ in Equation~(\ref{eqn:PIT}).  
\begin{description}
\item[Laplace margins] G is the Laplace distribution function and $\bY$ are marginally Laplace distributed.  Conditional on variable $Y_i$ exceeding a high threshold $t$, the Heffernan and Tawn model for the remaining variables $\bY_{-i}$ takes the form:
\begin{eqnarray}\label{eqn:HTlaplace}
  \bY_{-i} = \balpha_{|i} Y_i + (Y_i)^{\bbeta_{|i}}\bZ_{|i}
\end{eqnarray}
where $\bZ_{|i}$ is a vector residual and $(d-1)$ dimensional parameter vectors $\balpha_{|i}$ and $\bbeta_{|i}$ satisfy $(\balpha_{|i},\bbeta_{|i}) \epsilon [-1, 1]^{d-1}\times(-\infty, 1)^{d-1}$. Here, $\alpha_{j|i}$, the $\balpha_{|i}$ associated with $Y_j, (j\,\epsilon\, \{1,\ldots,d\}, j\neq i)$, then $0 < \alpha_{j|i}\leq 1$ and
$-1 \leq \alpha_{j|i} < 0$ correspond respectively to positive and negative association of $Y_j$ and large $Y_i$.
\item[Gumbel margins] G is the Gumbel distribution function and $\bY$ are marginally Gumbel distributed.  Conditional on variable $Y_i$ exceeding a high threshold $t$, the Heffernan and Tawn model for the remaining variables $\bY_{-i}$ takes the form:
\begin{eqnarray}\label{eqn:HTgumbel}
  \bY_{-i} = \balpha_{|i} Y_i + I_{\balpha_{|i}=0,\bbeta_{|i}<0}(\bc_{|i}
- \bd_{|i} \log Y_i)+ (Y_i)^{\bbeta_{|i}}\bZ_{|i}
\end{eqnarray}
where $\bZ_{|i}$ is a vector residual and $(d-1)$ dimensional parameter vectors $\balpha_{|i}$, $\bbeta_{|i}$, $\bc_{|i}$ and $\bd_{|i}$ this time satisfy $(\balpha_{|i},\bbeta_{|i}, \bc_{|i}, \bd_{|i}) \,\epsilon \,[0, 1]^{d-1}\times(-\infty, 1)^{d-1} \times (\infty,\infty)\times(0,1)$.  Here positive association between $Y_j$ and large $Y_i$ is described by $\alpha_{j|i}$, when $\alpha_{j|i}>0$ and $\beta_{j|i}<0$.  The  model structure changes in the case of negative dependence in which case $\alpha_{j|i}=0$ and further parameters $c_{j|i}$ and $d_{j|i}$ are required.
\end{description}
The structure of the dependence model is greatly simplified under the use of Laplace margins, in which case a single model structure suffices to describe both positive and negative dependence.  This makes inference considerably more straightforward, partcularly in the case of weak dependence.

Note that in both Laplace and Gumbel cases, there is no parametric family of distributions assumed to describe the distribution of model residuals $\bZ_{|i}$.  Thus the Heffernan and Tawn condtional dependence model is semi-parametric.  The dependence between conditioning variable $Y_i$ and the remaining variables $\bY_{-i}$ is described by a parametric regresion type model (either~(\ref{eqn:HTlaplace}) or~(\ref{eqn:HTgumbel})) and the distribution of the model residuals $\bZ_{|i}$ is modelled by the empirical distribution of observed model residuals.  These are calculated by using transformed data $\bY$ and estimates of model parameters $\hat\balpha,\hat\bbeta$ (and possibly also $\hat\bc$ and $\hat\bd$) in~(\ref{eqn:HTlaplace}) or~(\ref{eqn:HTgumbel}).
%
\subsubsection{Constraints on parameter space}
%
Recent developments to the Heffernan and Tawn method (\cite{KeefEtAl2011}) address the issue of validity of the fitted model.  This work shows that in order for the fitted model to be valid, it is necessary impose tighter constraints on the parameters of the Heffernan and Tawn model than the originl box constraints described above.  Constraints suggested by Keef {\it et al.}\ enforce the consistency of the fitted dependence model with the strength of extremal dependence exhibited by the data.

The effect of these constraints is to limit the shape of the dependence parameter space so that its boundary is curved. The constraint brings with it some performance issues for the optimiser used to estimate the dependence parameters, in particular sensitivity to choice of starting value.

In {\tt texmex}, this constrained estimation is implemented for Laplace margins only.  It is to be preferred to the use of unconstrained estimation which can result in invalid, inconsistent inferences and which can lead to misleading predictions particularly if extrapolation is to be made far into the tail of the modelled distribution.  As such, the package defaults are to use Laplace margins and to constrain the parameters to give valid fitted models.
%
\subsection{Conditional multivariate extreme value modelling using {\tt texmex}}
%
The whole conditional multivariate extreme value modelling
algorithm is rather complicated. Fitted models are arguably most easily interpreted by using them to predict quantities of interest. The implementation
in {\tt texmex} makes the whole thing quite easy, except that
for our particular example we need to account for the baseline
effect and transform predictions back to the scale of the raw
data.
%
\subsubsection{Model fitting and model diagnostics}
%
Now we will fit the multivariate model to the residuals from the previous baseline modelling of the {\tt liver} data.  Here, {\tt mqu} specifies the marginal quantile which defines the threshold above which the GPD models will be fitted.  By default, if no dependence threshold is supplied, the threshold for fitting the dependence component of the model is taken to be equal to that used to fit the GPD model to the tail of the conditioning variable. In what follows, the conditioning variable is {\tt ralt}.

<<mex>>=
Amex <- mex(Adat, mqu=.7, which="ralt")
Bmex <- mex(Bdat, mqu=.7, which="ralt")
Cmex <- mex(Cdat, mqu=.7, which="ralt")
Dmex <- mex(Ddat, mqu=.7, which="ralt")
@

The function {\tt mex} is a wrapper for the functions {\tt migpd} and {\tt mexDependence} which carry out the marginal and dependence modelling stages respectively.  An equivalent way of carrying out the above for groups A and D would be to use:

<<migpd.mexDependence>>=
Amarg <- migpd(Adat, mqu=0.7)
Dmarg <- migpd(Ddat, mqu=0.7)
Amex <- mexDependence(Amarg, which = "ralt")
Dmex <- mexDependence(Dmarg, which = "ralt")
@

We can check the diagnostics for the fitted marginal models in the usual way (here output is supressed since it is lengthy!).  Use of {\tt mrlPlot} can also be informative at this stage.
<<plot.migpd, eval=FALSE>>=
par(mfrow=c(2,2))
plot(Amarg)
plot(Dmarg)
@

Plotting model diagnostics for the dependence component of the model is carried out as follows:

<<mexA, fig=TRUE>>=
par(mfcol=c(3, 3))
plot(Amex)
@

<<mexD, fig=TRUE>>=
par(mfcol=c(3, 3))
plot(Dmex)
@

The plots show (top to bottom): centred and scaled values of the 
dependence model residuals across
the range of the extreme conditioning variable; absolute
values of these; and the original untransformed data with contours 
showing quatiles of the fitted conditional model (here this scale 
is that of the {\tt ralt, rast, ralp, rtbl} variables). If the 
model fits the data, the top and centre rows of the plots should show no
structure and the scatterplot smoothers should be more or
less horizontal.  In the bottom row, the fitted quantiles should 
agree with the shape of the raw data distribution.  Take care to 
note that the one dimensional conditional distribution of $(X_j \,|\, X_i)$ 
(whose estimated value is shown by the contours at different 
values of $X_i$) is \emph{not} the same thing as the (two dimensional) 
joint distribution of the $(X_i,X_j)$, estimated by the scatterplot 
of the data points.

For dose D, the centre plots in the top and middle rows give us mild 
cause for concern over model fit, since the smoothed lines are not 
absolutely flat. We can investigate further
by plotting the dependence structure parameter estimates
across a range of thresholds. Beyond a suitably high
threshold, we should expect the parameters to be constant.
To gain some feeling for the variability in the parameters,
we perform 10 (by default) bootstrap samples.  We set {\tt trace=11} 
to suppress printing of progress reports in this document (the 
default is to report every ten replicates).

<<mexRangeFit, fig=TRUE>>=
par(mfrow=c(3, 2))
mexRangeFit(Dmarg, "ralt", trace=11)
@

The resulting plot provides some reassurance that the $70^{th}$
percentile is a suitable threshold.

Before carrying on to examine our fitted models or to use them for prediction, we need to take some care to make sure our parameter estimates do correspond to the true maximum of the objective functions used for estimation.  This is an issue since the performance of the optimiser can be sensitive to the choice of starting value. It is up to the user to check that the parameter estimates have converged to the true maximum likelihood estimates.  This is carried out straightforwardly using simple visual diagnostics.  To reduce the amount of output produced, here we show the procedure only for ALT and AST at dose A.  We use {\tt mexDependence} to plot the profile-likelihood surface which is maximised for estimation of the dependence model parameters.  It is left as an exercise to produce plots for all of the conditional models fitted in this section here using {\tt mexDependence(Amarg, etc ...}

<<checkMax1,fig=TRUE>>=
par(mfrow=c(1,1))
Amarg.alt.ast <- migpd(Adat[,c("ralt","rast")],mqu=0.7)
Amex.alt.ast <- mexDependence(Amarg.alt.ast, which = "ralt", 
                              dqu=0.7, PlotLikDo=TRUE)
@

This plot shows the point estimate to lie on the edge of the parameter space, and we can home in on the region containing this estimate to check that the surface has been successfully maximised:

<<checkMax2,fig=TRUE>>=
par(mfrow=c(1,1))
Amex.alt.ast <- mexDependence(Amarg.alt.ast, which = "ralt", 
                              dqu=0.7, PlotLikDo=TRUE,
                              PlotLikRange=list(a=c(0.7,1.0),b=c(0.0,0.4) ))
@

This plot reassures us that the point estimate does correspond to the mle.  If this had not been the case, we should have tried a range of different starting values for the optimisation.  More details are given in the documentation for {\tt mexDependence}.

Finally, now that we are satisfied with the fit of our model, we can examine the estimated model parameters.  The parameters in the dependence structure are not straighforwardly interpretable, though values of {\tt a} close to 1 indicate
strong extremal dependence. 

<<mexPars>>=
Amex
Dmex
@

It is clear that under dose A, ALT and AST
are most closely related and under dose D, ALT and ALP are most closely related.



%
\subsubsection{Prediction under the fitted model}
%
Comparison of the plots of ALP vs ALT across the doses
reveals that the extremal dependence between the two variables
increases with dose (see Figure~\ref{fig:alt.alpData} on page~\pageref{fig:alt.alpData}).

We can get predictions out of the conditional multivariate
model using the {\tt predict} method. We tell the function
to simulate values of the variables conditional on {\tt ralt}
being above its $90^{th}$ percentile. Remembering that
we are working with residuals from a linear regression model,
we are about to simulate values of the variables for the 10\%
of patients with the largest increases in ALT.

<<predictA>>=
set.seed(20111013)
nsim <- 1000
pA <- predict(Amex, pqu=.9, nsim=nsim)
set.seed(20111013)
pB <- predict(Bmex, pqu=.9, nsim=nsim)
set.seed(20111013)
pC <- predict(Cmex, pqu=.9, nsim=nsim)
set.seed(20111013)
pD <- predict(Dmex, pqu=.9, nsim=nsim)
summary(pA)
@

The thresholds cited in the final part of the output are by default taken to be the marginal thresholds used for fitting the GPD models  (in this case these are the 0.7 quantiles of the marginal distributions).  However, any value of threshold can be used for prediction by specifying the argument {\tt mth} of the {\tt summary} function. 

<<plotPredictA, fig=TRUE>>=
par(mfrow=c(2, 2))
plot(pA)
@

Plots show the original data (light blue circles) and data simulated under the fitted model above the threshold for prediction (purple crosses).  The solid orange curve in each plot is for reference and joins equal quantiles of the marginal distributions -- perfectly dependent variables would lie exactly on this line (this line is analogous to the diagnonal line on a QQ plot, but here since the two marginal distributions are not equal, the curve is not a straight line). We can compare the output for group A with that for group D.

<<plotPredictD, fig=TRUE>>=
summary(pD)
par(mfrow=c(2, 2))
plot(pD)
@

There is a noticable difference between the relationship
between the simulated values of the residuals of ALT and ALP in the two dose 
groups, with stronger dependence in the higher dose group.

We can get a feel for what the model is telling us by
transforming the simulated data to the scale of the
raw data and plotting. Once more, we have to account for
baseline effects.

<<simData>>=
simData <- rbind(pA$data$simulated, pB$data$simulated, pC$data$simulated, pD$data$simulated)

altlin <- rlm(log(ALT.M) ~ log(ALT.B) + ndose, data=liver, c=3.44, method="MM")
astlin <- rlm(log(AST.M) ~ log(AST.B) + ndose, data=liver, c=3.44, method="MM")
alplin <- rlm(log(ALP.M) ~ log(ALP.B) + ndose, data=liver, c=3.44, method="MM")
tbllin <- rlm(log(TBL.M) ~ log(TBL.B) + ndose, data=liver, c=3.44, method="MM")

balt <- sample(liver$ALT.B, size=nsim*4, replace=TRUE)
bast <- sample(liver$AST.B, size=nsim*4, replace=TRUE)
balp <- sample(liver$ALP.B, size=nsim*4, replace=TRUE)
btbl <- sample(liver$TBL.B, size=nsim*4, replace=TRUE)

sdose <- rep(1:4, ea=nsim)

nd <- data.frame(ALT.B = balt, AST.B = bast, ALP.B = balp, TBL.B = btbl, ndose=sdose)

salt <- exp(predict(altlin, new=nd) + simData$ralt)
sast <- exp(predict(astlin, new=nd) + simData$rast)
salp <- exp(predict(alplin, new=nd) + simData$ralp)
stbl <- exp(predict(tbllin, new=nd) + simData$rtbl)
@

We can now plot the data and the simualted values. Following
current FDA guidance, we are primarily interested in the
possibility of combined ALT and TBL elevations. Specifically,
we are interested in cases where $ALT > 3\times ULN$ and
$TBL > 2\times ULN$, and we draw reference lines at these
values.

<<plotALTvsTBL, fig=TRUE>>=
par(mfrow=c(2, 2))
for (dose in 1:4){
    plot(liver$ALT.M[liver$ndose == dose], liver$TBL.M[liver$ndose == dose],
         xlim=range(liver$ALT.M[liver$ndose == dose], salt[sdose == dose]),
         ylim = range(liver$TBL.M[liver$ndose == dose], stbl[sdose == dose]),
         xlab="ALT (U/L)", ylab="TBL (umol/L)", main=LETTERS[dose],
         log="xy", type = "n")
    points(salt[sdose == dose], stbl[sdose == dose], col=4, pch=3)
    points(liver$ALT.M[liver$ndose == dose], liver$TBL.M[liver$ndose == dose],
           col=2, pch=16)
    abline(v = 3 * 36, lty=2, col=3)
    abline(h = 2 * 21, lty=2, col=3)
}
@

There does not appear to be a large probability of a combined
ALT and TBL elevation. Since we noticed an apparent relationship
between ALT and ALP, particularly on dose D, we do the same
plot for those variables. We draw a horizontal reference line
at the upper limit of normal of ALP, 130 U/L.

<<plotALTvsALP, fig=TRUE>>=
par(mfrow=c(2, 2))
for (dose in 1:4){
    plot(liver$ALT.M[liver$ndose == dose], liver$ALP.M[liver$ndose == dose],
         xlim=range(liver$ALT.M[liver$ndose == dose], salt[sdose == dose]),
         ylim = range(liver$ALP.M[liver$ndose == dose], salp[sdose == dose]),
         xlab="ALT (U/L)", ylab="ALP (u/L)", main=LETTERS[dose],
         log="xy", type = "n")
    points(salt[sdose == dose], salp[sdose == dose], col=4, pch=3)
    points(liver$ALT.M[liver$ndose == dose], liver$ALP.M[liver$ndose == dose],
           col=2, pch=16)
    abline(v = 3 * 36, lty=2, col=3)
    abline(h = 130, lty=2, col=3)
}
@

The effect of dose on the strength of extremal dependence between ALT and ALP is clearly apparent
on the scale of the raw data. 

\bibliography{extremeLiver}
\bibliographystyle{plain}
\end{document}


